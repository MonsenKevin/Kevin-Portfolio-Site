<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Kevin Monsen Portfolio</title>
    <link rel="canonical" href="https://kevinmonsen.com/fse.html">
    <link rel="stylesheet" href="./style.css">
    <script type="module" src="./main.js"></script>
</head>

<body>
    <div class="showcase">
        <section class="problem-solver">
            <div class="grid">

                <!-- Left: photo (optional) -->
                <figure class="leadership-media" aria-hidden="true">
                    <img src="./images/Engineer.png" alt="" />
                    <figcaption class="sr-only">Picture of Device</figcaption>
                </figure>

                <!-- Right: content -->
                <div class="content">
                    <h2 id="fse">FSE 100: AI Vision Project</h2>
                    <div class="metrics">
                        <span class="chip">Python</span>
                        <span class="chip">Raspberry Pi</span>
                        <span class="chip">Sensors</span>
                        <span class="chip">Gemini</span>
                    </div>
                    <p class="intro">
                        As part of an Introduction to Engineering course, collaborated on the design and construction of
                        a wearable assistance device intended to improve safety for visually impaired users. The goal
                        was to address hazards above waist levelâ€”an area not detectable by traditional white canes and a
                        common source of injury. <br> <br>
                        The system combined computer vision and proximity sensing to detect obstacles and provide
                        real-time feedback to the user. A camera captured images of the surrounding environment, which
                        were analyzed via an AI vision API to identify nearby obstacles. When an obstacle was detected,
                        a vibration motor provided tactile feedback to warn the user. <br> <br>
                        In parallel, an ultrasonic sensor continuously measured distance to nearby objects. If an object
                        entered a defined safety threshold, an audible buzzer was triggered to provide an additional
                        alert. This multi-sensor approach improved reliability by combining AI-based recognition with
                        direct distance measurement. <br> <br>
                        The final prototype demonstrated how layered sensing and feedback mechanisms can enhance user
                        awareness and reduce the risk of head-level collisions.  <br> <br>
                    </p>
                    <div class="cta-buttons">
                        <a href="https://github.com/MonsenKevin/AI-Vision-Assistance">View the GitHub</a>
                    </div>
                </div>
        </section>
    </div>
</body>

</html>